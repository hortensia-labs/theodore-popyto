## 4.2.1. Modelos Actuales y su Aplicación en Dominios Creativos {#modelos-iag-actuales}

La Inteligencia Artificial generativa (IAG) ha marcado un cambio de paradigma tecnológico en la última década. Mientras que la IA tradicional se enfocaba en analizar datos o resolver tareas específicas de clasificación, la nueva generación de modelos pre-entrenados ha transformado la IA en una tecnología de propósito general capaz de *crear* contenido original (Dahn, 2024). Este giro —de *clasificar* a *generar*— supone una transformación cualitativa: un sistema de visión por computadora podía reconocer la imagen de un bailarín, pero ahora un modelo generativo puede producir *nuevas* imágenes, textos o secuencias de movimiento que antes no existían.

Comprender el estado del arte de estas tecnologías resulta imprescindible para evaluar con rigor tanto sus capacidades actuales como sus limitaciones estructurales. En esta sección examinamos las principales arquitecturas que impulsan la IAG y sus aplicaciones emergentes en los dominios creativos, con especial atención a la generación de movimiento humano.

### Arquitecturas fundamentales de la IA generativa

Tres arquitecturas principales han impulsado el desarrollo de la IAG: los **Transformers**, las **Redes Generativas Antagónicas (GAN)** y los **Modelos de Difusión**. Cada una ha contribuido de manera distintiva a los logros actuales en generación de contenidos.

**Transformers**: Introducidos en 2017, los transformers son modelos de aprendizaje profundo basados en mecanismos de *self-attention* que revolucionaron el procesamiento de secuencias. Su capacidad para manejar contextos largos de información permitió un salto cualitativo en el entendimiento y generación de lenguaje natural. Modelos como GPT-3 y GPT-4 demostraron que un solo sistema entrenado con cantidades masivas de texto puede generar respuestas coherentes a prácticamente cualquier consulta (Dahn, 2024). Esto supuso pasar de sistemas entrenados para tareas puntuales a **modelos generales** capaces de desempeñar múltiples funciones que nunca fueron programadas explícitamente.

En el ámbito creativo, los transformers dominan la generación de texto (poemas, ensayos, guiones, código) y también se aplican a música (como *Jukebox* de OpenAI) y, combinados con otras técnicas, a la generación de movimientos. La clave de los transformers es que aprenden **relaciones contextuales** complejas, otorgándoles flexibilidad y coherencia en la generación secuencial.

**Redes Generativas Antagónicas (GAN)**: Propuestas en 2014 por Ian Goodfellow, las GAN fueron durante años la técnica punta para generar imágenes y audio sintéticos. En una GAN se entrenan dos redes neuronales en competencia: un *generador* que crea muestras falsas imitando datos reales, y un *discriminador* que intenta distinguir las muestras generadas de las auténticas. Ambos mejoran juntos en un "juego adversarial" hasta que el generador produce ejemplos tan realistas que el discriminador no puede diferenciarlos (Farid, 2024). Un caso paradigmático fue **StyleGAN** de NVIDIA, que generó rostros humanos ficticios con calidad fotográfica.

Las GAN lograron resultados sorprendentes en artes visuales, audio e incluso video de corta duración. Sin embargo, presentan **limitaciones**: el proceso de entrenamiento es inestable y carecen de control directo sobre atributos específicos de la salida. Una GAN entrenada para crear rostros produce caras realistas, pero es difícil indicarle que genere un rostro con rasgos particulares sin intervenciones adicionales (Farid, 2024). En el ámbito del movimiento, algunas investigaciones han empleado GANs para generar secuencias de baile sincronizadas con música; un estudio reciente (Lin y Feng, 2025) entrenó una GAN sobre el dataset AIST++ logrando que el 85% de bailarines profesionales evaluaran las secuencias generadas como altamente consistentes con el estilo real.

**Modelos de Difusión**: Emergidos a partir de 2020, los modelos de difusión se consolidaron rápidamente como tecnología estrella para la generación de imágenes, superando a las GAN en calidad y control. El proceso de difusión consiste en aprender a invertir una trayectoria de ruido: el modelo entrena corrompiendo imágenes reales añadiéndoles ruido progresivamente, y aprende luego a "desenruidecer" paso a paso (Farid, 2024). En síntesis, aprende cómo ir desde ruido aleatorio hasta una imagen coherente, lo que le permite generar imágenes desde la nada aplicando el proceso inverso.

Sistemas como **Stable Diffusion**, **DALL-E 2** o **Imagen** de Google permiten generar imágenes de alta fidelidad condicionadas a *prompts* textuales. La difusión se ha extendido también a **audio** (generación de voz o música) e incluso a **video**. En 2022-2023 aparecieron los primeros prototipos de texto-a-video, inicialmente con resultados rudimentarios, pero los avances han sido acelerados: en 2024-2025 se logran clips de varios segundos con mayor realismo y consistencia temporal (Farid, 2024). Y, como veremos, la arquitectura de difusión también ha sido aplicada con éxito a la **generación de movimiento humano**.

### Generación de movimiento humano y danza

Los últimos años han visto emerger modelos específicamente diseñados para generar movimientos humanos de manera autónoma. Estos sistemas suelen entrenarse con datos de *motion capture* (MoCap) o animaciones 3D de bailarines, para luego producir nuevas secuencias de movimiento.

**EDGE (Editable Dance Generation)**: Desarrollado en 2023 por investigadores de Stanford, EDGE es un sistema que crea coreografías enteras ajustadas a una pieza musical dada (Myers, 2023). Combina un transformer difusivo (modelo de difusión condicionado por música, con mecanismos de atención) con técnicas de edición de movimiento. Su característica innovadora es que las danzas generadas son *editables*: el coreógrafo puede especificar poses clave y el sistema auto-completa el resto de forma realista y fluida. Según sus creadores, las secuencias producidas son "musicalmente acertadas y físicamente plausibles", hasta el punto de que un bailarín humano podría ejecutarlas en la realidad (Myers, 2023). En evaluaciones con observadores humanos, EDGE fue preferido ampliamente sobre métodos anteriores.

**Modelos de danza condicionada a música**: Previo a EDGE, se propusieron sistemas como **Bailando** (2022) o **FACT** (2022), que utilizaban redes neuronales recurrentes o transformers para generar baile a partir de una canción, aunque con resultados más limitados. Los modelos recientes aprovechan la potencia de la difusión; Huang et al. (2023) presentaron un *Motion Diffusion Model* para secuencias de baile sincronizadas con música, y Liu et al. (2025) introdujeron técnicas de *curriculum learning* para generar baile a largo plazo con estabilidad.

**Generación de dúos e improvisación interactiva**: Okupnik et al. (2025) desarrollaron un modelo interactivo basado en difusión que actúa como "compañero artificial" capaz de responder en tiempo real al movimiento de un bailarín humano (Robbins & Sandberg, 2023). El sistema aprende a imitar parcialmente los movimientos del humano (manteniendo sincronización en componentes de baja frecuencia) pero introduce variaciones propias en componentes de alta frecuencia. El resultado es un avatar que baila *junto* al humano, manteniendo cierto alineamiento pero improvisando diferencias, simulando así una interacción no verbal hombre-máquina.

### Aplicaciones en industrias creativas

La IAG ya impacta múltiples sectores creativos:

**Artes visuales**: Artistas como Refik Anadol han utilizado modelos generativos para instalaciones como *Unsupervised* (MoMA, 2022), donde la IA entrenada con el archivo del museo crea visualizaciones dinámicas en constante cambio. Herramientas como Midjourney, DALL-E o Stable Diffusion permiten crear arte digital con solo describir una idea. En 2022, una imagen generada con Midjourney ganó un concurso de arte digital, desatando debates sobre la consideración del software como coautor.

**Música y audio**: Herramientas como **MusicLM** de Google (2023) generan fragmentos musicales coherentes de varios minutos a partir de descripciones textuales. En 2023, una canción con voces sintéticas de Drake y The Weeknd (*Heart on My Sleeve*) se volvió viral antes de ser retirada por cuestiones de derechos (Snapes, 2023). Este caso demostró que es técnicamente posible imitar timbres vocales y estilos de artistas consagrados con la IA actual.

**Cine y audiovisual**: En pre-producción, la IA genera *storyboards* a partir de guiones o concept art rápido mediante modelos text-to-image. En post-producción, se emplean técnicas generativas para efectos visuales, *deepfakes* o sincronización labial en doblaje. La huelga de 2023 de guionistas y actores en Hollywood tuvo como tema central el uso de IAG y la protección de los profesionales humanos (Farid, 2024).

**Videojuegos**: La generación procedural de terrenos, mapas y misiones mediante algoritmos aprendidos aporta variedad. Estudios emplean modelos generativos para crear texturas o diseños de personajes desde descripciones. En animación, la IA (como la ofrecida por *DeepMotion*) convierte video a animación 3D automáticamente. Sin embargo, han surgido controversias: bailarines demandaron a compañías de videojuegos por uso de sus movimientos virales sin permiso.

**Danza y performance experimental**: En 2024 se estrenó en Francia *Lilith.Aeon*, anunciada como la primera producción escénica con una IA como intérprete (Winship, 2024). "Lilith" es un ente virtual cuyo baile es generado en tiempo real; los movimientos del público alrededor del cubo LED influyen en su danza. Los coreógrafos alimentaron a la IA con un "diccionario" de movimientos, y luego la IA combinó y generó nuevas "palabras" de movimiento no imaginadas por los humanos.

Wayne McGregor ha explorado la IA en danza desde hace más de una década. En piezas como *Autobiography* (2019), la IA recombinaba el material de forma distinta cada función (Winship, 2024). Alexander Whitley trabaja en integrar los movimientos del público mediante sensores para que avatares virtuales bailen influenciados por la audiencia. Nicole Seiler estrenó en 2023 *Human in the Loop*, donde dos bailarines ejecutan en vivo la coreografía que una IA genera y les dicta en tiempo real.

### Plausibilidad física y expresividad: avances y brechas

Los progresos en **plausibilidad física** son notables. Modelos como EDGE integran en el entrenamiento restricciones para evitar artefactos como *foot sliding* (pies que resbalan sin motivo) o poses anatómicamente dudosas. El modelo de Lin y Feng (2025) incorporó mecanismos de sincronización musical y estructura coreográfica que garantizaron que los pasos generados cayeran a tiempo con la música y reflejaran su emoción, manteniendo coherencia estilística.

No obstante, la **expresividad artística profunda** resulta más difícil de cuantificar. Aunque una IA logre imitar el estilo de un tango o un ballet (su vocabulario de pasos, su timing), permanece abierta la pregunta de si transmite la *intención* o la *emoción* de un bailarín humano. Los modelos se evalúan en términos de realismo y compatibilidad con música, más que por su capacidad de evocar sentimientos en la audiencia. Esta brecha entre corrección técnica y significado artístico constituye, como veremos en la siguiente sección, una limitación estructural de la tecnología actual.

---

[NOTA: Los avances en IAG son muy rápidos. Verificar que las referencias (2023-2025) sigan siendo "estado del arte" al momento de la defensa de la tesis.]

[NOTA: Posible necesidad de investigación adicional sobre modelos emergentes de generación de movimiento multimodal (texto+audio+video→movimiento) si surgen desarrollos significativos post-2024.]
